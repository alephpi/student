{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Tl0IvjYCv6XS"],"mount_file_id":"1Yv1yMnlTFe7TV05bdyifHv2HTV6aut5O","authorship_tag":"ABX9TyO7NGECW/FSf6Lrln3UdKYW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## install hugging face"],"metadata":{"id":"ekK14hXuNcUc"}},{"cell_type":"code","source":["! pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RNlzpmXxNZBm","executionInfo":{"status":"ok","timestamp":1671118102121,"user_tz":-60,"elapsed":7319,"user":{"displayName":"sicheng mao","userId":"10284342385280709430"}},"outputId":"6f9c7ded-3729-495d-ddb6-006ccc874fdf"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n"]}]},{"cell_type":"markdown","source":["## load sequence features"],"metadata":{"id":"iKctEPD8v_f1"}},{"cell_type":"code","source":["%cd drive/MyDrive/altegrad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JtkMw20SDsoo","executionInfo":{"status":"ok","timestamp":1671118093771,"user_tz":-60,"elapsed":384,"user":{"displayName":"sicheng mao","userId":"10284342385280709430"}},"outputId":"cecdd5dc-5844-48b0-a2cc-d913fd5b4f61"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/altegrad\n"]}]},{"cell_type":"code","source":["import csv\n","import numpy as np\n","from sklearn.metrics import accuracy_score, log_loss\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","\n","# Read sequences\n","sequences = list()\n","with open('./data/sequences.txt', 'r') as f:\n","    for line in f:\n","        sequences.append(line[:-1])\n","\n","# Split data into training and test sets\n","sequences_train = list()\n","sequences_test = list()\n","proteins_test = list()\n","y_train = list()\n","with open('./data/graph_labels.txt', 'r') as f:\n","    for i,line in enumerate(f):\n","        t = line.split(',')\n","        if len(t[1][:-1]) == 0:\n","            proteins_test.append(t[0])\n","            sequences_test.append(sequences[i])\n","        else:\n","            sequences_train.append(sequences[i])\n","            y_train.append(int(t[1][:-1]))"],"metadata":{"id":"GTxvX0elv-2G","executionInfo":{"status":"ok","timestamp":1671118582237,"user_tz":-60,"elapsed":1584,"user":{"displayName":"sicheng mao","userId":"10284342385280709430"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["import os, psutil; print(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lWEiZalr_0BT","executionInfo":{"status":"ok","timestamp":1671118589409,"user_tz":-60,"elapsed":439,"user":{"displayName":"sicheng mao","userId":"10284342385280709430"}},"outputId":"9568e455-fcc2-4a73-b958-81b1134a5d69"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["3863.97265625\n"]}]},{"cell_type":"markdown","source":["## load pretrained model"],"metadata":{"id":"nT9-GX6vOn6F"}},{"cell_type":"code","source":["from transformers import EsmTokenizer, EsmModel\n","import torch\n","from tqdm import tqdm\n","\n","tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n","model = EsmModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qyz1E-0uOnFR","executionInfo":{"status":"ok","timestamp":1671118525122,"user_tz":-60,"elapsed":8762,"user":{"displayName":"sicheng mao","userId":"10284342385280709430"}},"outputId":"79d8c6bc-fd56-4ee4-84ee-8b6cc2fa3ec7"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'esm.contact_head.regression.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'esm.contact_head.regression.bias']\n","- This IS expected if you are initializing EsmModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing EsmModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.weight', 'esm.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["seq_batch = np.split(np.array(sequences_train, dtype='str'), range(0,len(sequences_train),8))\n","seq_batch.pop(0)\n","seq_batch = [list(array) for array in seq_batch]"],"metadata":{"id":"DLm3N9WR6b6I","executionInfo":{"status":"ok","timestamp":1671115438639,"user_tz":-60,"elapsed":9,"user":{"displayName":"sicheng mao","userId":"10284342385280709430"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["X_train = np.array([])"],"metadata":{"id":"pCuWsKlb9LRH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embeddings = []"],"metadata":{"id":"u2PdpExC9RXx","executionInfo":{"status":"ok","timestamp":1671116672304,"user_tz":-60,"elapsed":2,"user":{"displayName":"sicheng mao","userId":"10284342385280709430"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  for seq in tqdm(sequences_train):\n","    inputs = tokenizer(seq, return_tensors=\"pt\")\n","    outputs = model(**inputs)\n","    # print(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)\n","    embedding = outputs.last_hidden_state#.sum(axis=1)\n","    embeddings.append(embedding)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"koPOF7pSwzRf","executionInfo":{"status":"ok","timestamp":1671117711220,"user_tz":-60,"elapsed":1037644,"user":{"displayName":"sicheng mao","userId":"10284342385280709430"}},"outputId":"59f11a1c-de86-476b-d5fb-451d1d048e6b"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4888/4888 [17:17<00:00,  4.71it/s]\n"]}]},{"cell_type":"code","source":["X_train = []\n","for embedding in embeddings:\n","  X_train.append(embedding.squeeze())"],"metadata":{"id":"H6JO1NGlZve7","executionInfo":{"status":"ok","timestamp":1671118271207,"user_tz":-60,"elapsed":4,"user":{"displayName":"sicheng mao","userId":"10284342385280709430"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import pickle\n","with open('./embedding/X_train_esm_embedding_AA', 'wb') as f:\n","  pickle.dump(X_train, f)"],"metadata":{"id":"t3D2qUxPaUEf","executionInfo":{"status":"ok","timestamp":1671118386275,"user_tz":-60,"elapsed":10374,"user":{"displayName":"sicheng mao","userId":"10284342385280709430"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["embeddings = []"],"metadata":{"id":"chylDaUBbV2u","executionInfo":{"status":"ok","timestamp":1671118567237,"user_tz":-60,"elapsed":356,"user":{"displayName":"sicheng mao","userId":"10284342385280709430"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  for seq in tqdm(sequences_test):\n","    inputs = tokenizer(seq, return_tensors=\"pt\")\n","    outputs = model(**inputs)\n","    embedding = outputs.last_hidden_state\n","    embeddings.append(embedding)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671118850379,"user_tz":-60,"elapsed":248189,"user":{"displayName":"sicheng mao","userId":"10284342385280709430"}},"outputId":"c7898668-5fd6-4ab3-d5eb-eecbe03e3fd2","id":"hw2KTeeXMDVk"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1223/1223 [04:07<00:00,  4.93it/s]\n"]}]},{"cell_type":"code","source":["X_test = []\n","for embedding in embeddings:\n","  X_test.append(embedding.squeeze())"],"metadata":{"executionInfo":{"status":"ok","timestamp":1671118851969,"user_tz":-60,"elapsed":6,"user":{"displayName":"sicheng mao","userId":"10284342385280709430"}},"id":"ga1dxlnUbriw"},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["import pickle\n","with open('./embedding/X_test_esm_embedding_AA', 'wb') as f:\n","  pickle.dump(X_test, f)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1671118854726,"user_tz":-60,"elapsed":2761,"user":{"displayName":"sicheng mao","userId":"10284342385280709430"}},"id":"LvTPUcM8bri3"},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["## load graph features"],"metadata":{"id":"Tl0IvjYCv6XS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fygVcfKsfpeV"},"outputs":[],"source":["import csv\n","import time\n","import numpy as np\n","import scipy.sparse as sp\n","from sklearn.metrics import accuracy_score, log_loss\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","\n","def load_data(dir_name): \n","    \"\"\"\n","    Function that loads graphs\n","    \"\"\"  \n","    graph_indicator = np.loadtxt(dir_name+\"graph_indicator.txt\", dtype=np.int64)\n","    _,graph_size = np.unique(graph_indicator, return_counts=True)\n","    \n","    edges = np.loadtxt(dir_name+\"edgelist.txt\", dtype=np.int64, delimiter=\",\")\n","    A = sp.csr_matrix((np.ones(edges.shape[0]), (edges[:,0], edges[:,1])), shape=(graph_indicator.size, graph_indicator.size))\n","    A += A.T\n","    \n","    x = np.loadtxt(dir_name+\"node_attributes.txt\", delimiter=\",\")\n","    edge_attr = np.loadtxt(dir_name+\"edge_attributes.txt\", delimiter=\",\")\n","    \n","    adj = []\n","    features = []\n","    edge_features = []\n","    idx_n = 0\n","    idx_m = 0\n","    for i in range(graph_size.size):\n","        adj.append(A[idx_n:idx_n+graph_size[i],idx_n:idx_n+graph_size[i]])\n","        edge_features.append(edge_attr[idx_m:idx_m+adj[i].nnz,:])\n","        features.append(x[idx_n:idx_n+graph_size[i],:])\n","        idx_n += graph_size[i]\n","        idx_m += adj[i].nnz\n","\n","    return adj, features, edge_features\n","\n","def normalize_adjacency(A):\n","    \"\"\"\n","    Function that normalizes an adjacency matrix\n","    \"\"\"\n","    n = A.shape[0]\n","    A = A + sp.identity(n)\n","    degs = A.dot(np.ones(n))\n","    inv_degs = np.power(degs, -1)\n","    D = sp.diags(inv_degs)\n","    A_normalized = D.dot(A)\n","\n","    return A_normalized\n","\n","def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n","    \"\"\"\n","    Function that converts a Scipy sparse matrix to a sparse Torch tensor\n","    \"\"\"\n","    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n","    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n","    values = torch.from_numpy(sparse_mx.data)\n","    shape = torch.Size(sparse_mx.shape)\n","    return torch.sparse.FloatTensor(indices, values, shape)\n","\n","\n","class GNN(nn.Module):\n","    \"\"\"\n","    Simple message passing model that consists of 2 message passing layers\n","    and the sum aggregation function\n","    \"\"\"\n","    def __init__(self, input_dim, hidden_dim, dropout, n_class):\n","        super(GNN, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, n_class)\n","        self.bn = nn.BatchNorm1d(hidden_dim)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x_in, adj, idx):\n","        # first message passing layer\n","        x = self.fc1(x_in)\n","        x = self.relu(torch.mm(adj, x))\n","        x = self.dropout(x)\n","\n","        # second message passing layer\n","        x = self.fc2(x)\n","        x = self.relu(torch.mm(adj, x))\n","        \n","        # sum aggregator\n","        idx = idx.unsqueeze(1).repeat(1, x.size(1))\n","        out = torch.zeros(torch.max(idx)+1, x.size(1)).to(x_in.device)\n","        out = out.scatter_add_(0, idx, x)\n","        \n","        # batch normalization layer\n","        out = self.bn(out)\n","\n","        # mlp to produce output\n","        out = self.relu(self.fc3(out))\n","        out = self.dropout(out)\n","        out = self.fc4(out)\n","\n","        return F.log_softmax(out, dim=1)\n"]},{"cell_type":"code","source":["# Load graphs\n","adj, features, edge_features = load_data('./data/') "],"metadata":{"id":"eTS-biuHh9ls"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(adj),len(features),len(edge_features)"],"metadata":{"id":"gML_R_JFh_XE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# "],"metadata":{"id":"LfB6lAdugCP9"}},{"cell_type":"code","source":["# Normalize adjacency matrices\n","adj = [normalize_adjacency(A) for A in adj]\n","\n","# Split data into training and test sets\n","adj_train = list()\n","features_train = list()\n","y_train = list()\n","adj_test = list()\n","features_test = list()\n","proteins_test = list()\n","with open('graph_labels.txt', 'r') as f:\n","    for i,line in enumerate(f):\n","        t = line.split(',')\n","        if len(t[1][:-1]) == 0:\n","            proteins_test.append(t[0])\n","            adj_test.append(adj[i])\n","            features_test.append(features[i])\n","        else:\n","            adj_train.append(adj[i])\n","            features_train.append(features[i])\n","            y_train.append(int(t[1][:-1]))\n","\n","# Initialize device\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","# Hyperparameters\n","epochs = 50\n","batch_size = 64\n","n_hidden = 64\n","n_input = 86\n","dropout = 0.2\n","learning_rate = 0.001\n","n_class = 18\n","\n","# Compute number of training and test samples\n","N_train = len(adj_train)\n","N_test = len(adj_test)\n","\n","# Initializes model and optimizer\n","model = GNN(n_input, n_hidden, dropout, n_class).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","loss_function = nn.CrossEntropyLoss()\n","\n","# Train model\n","for epoch in range(epochs):\n","    t = time.time()\n","    model.train()\n","    train_loss = 0\n","    correct = 0\n","    count = 0\n","    # Iterate over the batches\n","    for i in range(0, N_train, batch_size):\n","        adj_batch = list()\n","        features_batch = list()\n","        idx_batch = list()\n","        y_batch = list()\n","        \n","        # Create tensors\n","        for j in range(i, min(N_train, i+batch_size)):\n","            n = adj_train[j].shape[0]\n","            adj_batch.append(adj_train[j]+sp.identity(n))\n","            features_batch.append(features_train[j])\n","            idx_batch.extend([j-i]*n)\n","            y_batch.append(y_train[j])\n","            \n","        adj_batch = sp.block_diag(adj_batch)\n","        features_batch = np.vstack(features_batch)\n","\n","        adj_batch = sparse_mx_to_torch_sparse_tensor(adj_batch).to(device)\n","        features_batch = torch.FloatTensor(features_batch).to(device)\n","        idx_batch = torch.LongTensor(idx_batch).to(device)\n","        y_batch = torch.LongTensor(y_batch).to(device)\n","        \n","        optimizer.zero_grad()\n","        output = model(features_batch, adj_batch, idx_batch)\n","        loss = loss_function(output, y_batch)\n","        train_loss += loss.item() * output.size(0)\n","        count += output.size(0)\n","        preds = output.max(1)[1].type_as(y_batch)\n","        correct += torch.sum(preds.eq(y_batch).double())\n","        loss.backward()\n","        optimizer.step()\n","    \n","    if epoch % 5 == 0:\n","        print('Epoch: {:03d}'.format(epoch+1),\n","              'loss_train: {:.4f}'.format(train_loss / count),\n","              'acc_train: {:.4f}'.format(correct / count),\n","              'time: {:.4f}s'.format(time.time() - t))\n","        \n","# Evaluate model\n","model.eval()\n","y_pred_proba = list()\n","# Iterate over the batches\n","for i in range(0, N_test, batch_size):\n","    adj_batch = list()\n","    idx_batch = list()\n","    features_batch = list()\n","    y_batch = list()\n","    \n","    # Create tensors\n","    for j in range(i, min(N_test, i+batch_size)):\n","        n = adj_test[j].shape[0]\n","        adj_batch.append(adj_test[j]+sp.identity(n))\n","        features_batch.append(features_test[j])\n","        idx_batch.extend([j-i]*n)\n","        \n","    adj_batch = sp.block_diag(adj_batch)\n","    features_batch = np.vstack(features_batch)\n","\n","    adj_batch = sparse_mx_to_torch_sparse_tensor(adj_batch).to(device)\n","    features_batch = torch.FloatTensor(features_batch).to(device)\n","    idx_batch = torch.LongTensor(idx_batch).to(device)\n","\n","    output = model(features_batch, adj_batch, idx_batch)\n","    y_pred_proba.append(output)\n","    \n","y_pred_proba = torch.cat(y_pred_proba, dim=0)\n","y_pred_proba = torch.exp(y_pred_proba)\n","y_pred_proba = y_pred_proba.detach().cpu().numpy()\n","\n","# Write predictions to a file\n","with open('sample_submission.csv', 'w') as csvfile:\n","    writer = csv.writer(csvfile, delimiter=',')\n","    lst = list()\n","    for i in range(18):\n","        lst.append('class'+str(i))\n","    lst.insert(0, \"name\")\n","    writer.writerow(lst)\n","    for i, protein in enumerate(proteins_test):\n","        lst = y_pred_proba[i,:].tolist()\n","        lst.insert(0, protein)\n","        writer.writerow(lst) "],"metadata":{"id":"QDC0zZbBgAhy"},"execution_count":null,"outputs":[]}]}