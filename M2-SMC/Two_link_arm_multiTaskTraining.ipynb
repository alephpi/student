{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c40416-2a80-48a1-9da6-b8287cb47cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "        'n_tasks': 1000,\n",
    "        'n_episode': 150,\n",
    "        'n_timesteps': 10,\n",
    "        'input_dim': 4, \n",
    "        'hidden_dim': 512,\n",
    "        'output_dim': 2,\n",
    "        'lr': 1e-4,\n",
    "        'batch_size': 128,\n",
    "        'n_epochs': 100,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiTA-Cwr5Pey"
   },
   "source": [
    "# Data Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoArmLinkDataset(Dataset):\n",
    "\t# `set_seed=None` means no seed fixed.  \n",
    "\tdef __init__(self, n_tasks=hyperparams['n_tasks'], n_episode=hyperparams['n_episode'], \n",
    "\t\t\t\tn_timesteps=hyperparams['n_timesteps'], set_seed: Union[int, None]=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.x_0 = []\n",
    "\t\tself.u = []\n",
    "\t\tself.x = []\n",
    "\t\tself.z = []\n",
    "\t\tif set_seed is not None:\n",
    "\t\t\tnp.random.seed(set_seed)\n",
    "\t\tl_of_tasks = np.random.normal(loc=1, scale=0.3, size=(n_tasks,2))\n",
    "\t\tfor i in tqdm(range(n_tasks)):\n",
    "\t\t\tl = l_of_tasks[i]\n",
    "\t\t\tfor j in range(n_episode):\n",
    "\t\t\t\tx_0, u, x, z = TwoArmLinkDataset.generate_episode(l, n_timesteps)\n",
    "\t\t\t\tself.x_0.append(x_0)\n",
    "\t\t\t\tself.u.append(u)\n",
    "\t\t\t\tself.x.append(x)\n",
    "\t\t\t\tself.z.append(z)\n",
    "\t\tprint(\"Two Arm Link Dataset Generation Finished!\")\n",
    "\t\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tx_0 = self.x_0[index]\n",
    "\t\tu = self.u[index]\n",
    "\t\tx = self.x[index]\n",
    "\t\tz = self.z[index]\n",
    "\t\tsample = {\n",
    "\t\t\t\"init_pos\": torch.tensor(x_0),\n",
    "\t\t\t\"control\": torch.tensor(u),\n",
    "\t\t\t\"true_pos\": torch.tensor(x),\n",
    "\t\t\t\"noisy_pos\": torch.tensor(z),\n",
    "\t\t}\n",
    "\t\treturn sample\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.x_0)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef generate_episode(l, n_timesteps):\n",
    "\t\tdef x_of_q(q, l):\n",
    "\t\t\t# q: array (2,) containing the two angles\n",
    "\t\t\t# l: array (2,) containing the limb lengths \n",
    "\t\t\tx = l[0] * np.cos(q[0]) + l[1] * np.cos(q[0] + q[1])\n",
    "\t\t\ty = l[0] * np.sin(q[0]) + l[1] * np.sin(q[0] + q[1])\n",
    "\t\t\treturn np.array([x, y])\n",
    "\t\tq_0 = np.random.uniform(low= -np.pi, high= np.pi, size=2)\n",
    "\t\tx_0 = x_of_q(q_0, l=l)\n",
    "\t\tu = np.random.randn(2, n_timesteps)\n",
    "\t\tq = u.cumsum(axis=1) + q_0.reshape(-1,1)\n",
    "\t\tx = x_of_q(q, l=l)\n",
    "\t\tz = x + np.random.normal(0, scale=0.001, size=(2, n_timesteps))\n",
    "\t\treturn x_0, u, x, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(batch_size=hyperparams['batch_size'], n_tasks=hyperparams['n_tasks'], set_seed=0):\n",
    "    dataset = TwoArmLinkDataset(n_tasks=n_tasks, set_seed=set_seed)\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            pin_memory=True,\n",
    "                            drop_last=True,\n",
    "                            )\n",
    "    return data_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "433695f6-b72d-4a99-b13e-9d280b5e8a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim=hyperparams['hidden_dim']):\n",
    "        super(DropoutLayer, self).__init__()\n",
    "        self.mask = torch.ones(hidden_dim, dtype=torch.float32, requires_grad=False)\n",
    "        self.training = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.mask\n",
    "\n",
    "    def update(self, mask: np.ndarray):\n",
    "        assert mask.shape == self.mask.shape, f\"new mask shape should be {self.mask.shape} but giving {mask.shape}\"\n",
    "        self.mask = torch.Tensor(mask)\n",
    "        \n",
    "    def get(self):\n",
    "        return self.mask.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_dim=hyperparams['input_dim'], hidden_dim=hyperparams['hidden_dim'], \n",
    "                output_dim=hyperparams['output_dim']):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        self.lin2 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.lin3 = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        # dropout for pretraining and finetunning\n",
    "        self.dropout_for_GD = nn.Dropout(p=0.5)\n",
    "        # dropout for smc\n",
    "        self.dropout = DropoutLayer(hidden_dim=hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        if self.training:\n",
    "            x = self.dropout_for_GD(x)\n",
    "        else:\n",
    "            x = self.dropout(x)\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "        \n",
    "    def update_dropout_mask(self, mask: np.ndarray):\n",
    "        self.dropout.update(mask)\n",
    "\n",
    "    def get_dropout_mask(self):\n",
    "        return self.dropout.get()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Task Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:06<00:00, 154.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two Arm Link Dataset Generation Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_loader(n_tasks=1000, set_seed=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MyModel().to(device)\n",
    "model = model.double()\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['lr'])\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader=train_loader, nb_epochs=hyperparams['n_epochs']):\n",
    "    total_loss = []\n",
    "    for epoch in range(1, nb_epochs + 1): \n",
    "    \t# store the loss in each batch\n",
    "        losses = []\n",
    "        # use tqdm to better visualize the training process\n",
    "        with tqdm(data_loader, unit=\"batch\") as tepoch: # evaluate every batch\n",
    "            for data in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\") # custome the printed message in tqdm\n",
    "                optimizer.zero_grad()\n",
    "                control = data['control'].to(device)\n",
    "                true_pos = data['true_pos'].to(device)\n",
    "                loss = 0\n",
    "                output = data['init_pos'] # initialisation\n",
    "                for i in range(hyperparams['n_timesteps']):\n",
    "                    input_ = torch.cat((output, control[:,:,i]), dim=1)\n",
    "                    output = model.forward(input_)\n",
    "                    loss += criterion(output, true_pos[:,:,i])\n",
    "                loss /= hyperparams['n_timesteps']\n",
    "                loss.backward() # backward propagation\n",
    "                optimizer.step() # update parameters\n",
    "                losses.append(loss.item())\n",
    "                # custome what is printed in tqdm message\n",
    "                tepoch.set_postfix(loss=sum(losses)/len(losses))\n",
    "        total_loss.append(sum(losses)/len(losses))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1171/1171 [01:25<00:00, 13.72batch/s, loss=0.799]\n",
      "Epoch 2:  58%|█████▊    | 684/1171 [00:50<00:36, 13.48batch/s, loss=0.737]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-05fd159e23bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# return a list of loss in different epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-07e49d42cec8>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(data_loader, nb_epochs)\u001b[0m\n\u001b[0;32m     18\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_pos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_timesteps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# backward propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yunhao\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yunhao\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_loss = train() # return a list of loss in different epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save parameters and optimizer\n",
    "torch.save({\n",
    "    'state_dict': model.state_dict(),\n",
    "    'loss': total_loss,\n",
    "    'optimizer' : optimizer.state_dict(),\n",
    "}, 'Task1_multiTask.pth.tar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (lin1): Linear(in_features=4, out_features=512, bias=True)\n",
       "  (lin2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (lin3): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout_for_GD): Dropout(p=0.5, inplace=False)\n",
       "  (dropout): DropoutLayer()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel().to(device)\n",
    "checkpoint = torch.load('Task1_multiTask.pth.tar')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e9dd1ae481486d48c9db6dc9dc7a9a7f82b64eba274b60c6e858d1cb03bdb70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
