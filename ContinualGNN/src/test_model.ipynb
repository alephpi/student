{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive reimport\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, logging, time\n",
    "import os.path as osp\n",
    "import torch\n",
    "from model import Model\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description = 'pytorch version of GraphSAGE')\n",
    "parser.add_argument('--data', type = str, default = 'cora')\n",
    "# parser.add_argument('--aggr_func', type = str, default = 'MEAN') # dead argmument\n",
    "parser.add_argument('--num_epochs', type = int, default = 10)\n",
    "parser.add_argument('--batch_size', type = int, default = 128)\n",
    "parser.add_argument('--seed', type = int, default = 13)\n",
    "parser.add_argument('--cuda', action = 'store_true', help = 'use CUDA')\n",
    "parser.add_argument('--num_neg_samples', type = int, default = 10) # dead argument\n",
    "parser.add_argument('--lr', type = float, default = 1e-3)\n",
    "args = parser.parse_args(args=['--cuda'])\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data='cora', num_epochs=10, batch_size=128, seed=13, cuda=True, num_neg_samples=10, lr=0.001, device=device(type='cuda'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-15 18:27:42,597 - INFO - Device:cuda\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "logging.basicConfig(level = logging.INFO, format = '%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info('Device:' + str(args.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'cora' \n",
    "attributes_file_name = osp.join('../data', data_name, 'attributes')\n",
    "labels_file_name = osp.join('../data', data_name, 'labels')\n",
    "valid_file_name = osp.join('../data', data_name, 'valid_nodes')\n",
    "\n",
    "features = np.loadtxt(attributes_file_name, dtype=np.float32)\n",
    "labels = np.loadtxt(labels_file_name, dtype=np.int64)[:,1]\n",
    "valid_all_nodes_list = np.loadtxt(valid_file_name, dtype = np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_handler import update_viewed_all_nodes_and_edges, generate_whole_graph\n",
    "def load_graph(t=14):\n",
    "\tstream_edges_dir_name = osp.join('../data', data_name, 'stream_edges')\n",
    "\tviewed_all_nodes, viewed_all_edges = None, None\n",
    "\tfor tt in range(t):\n",
    "\t\tcoming_edges = np.loadtxt(osp.join(stream_edges_dir_name, str(tt)), dtype=int)\n",
    "\t\tviewed_all_nodes, viewed_all_edges = update_viewed_all_nodes_and_edges(\n",
    "\t\t\t\t\t\t\t\tcoming_edges, viewed_all_nodes, viewed_all_edges) \n",
    "\t\tgraph, valid_nodes = generate_whole_graph(viewed_all_nodes, viewed_all_edges, valid_all_nodes_list, features, labels)\n",
    "\treturn graph, valid_nodes\n",
    "\n",
    "graph, valid_nodes = load_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 13,\n",
       " 16,\n",
       " 19,\n",
       " 20,\n",
       " 24,\n",
       " 26,\n",
       " 27,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 34,\n",
       " 35,\n",
       " 39,\n",
       " 42,\n",
       " 56,\n",
       " 57,\n",
       " 60,\n",
       " 71,\n",
       " 72,\n",
       " 77,\n",
       " 83,\n",
       " 88,\n",
       " 91,\n",
       " 100,\n",
       " 103,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 120,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 127,\n",
       " 128,\n",
       " 131,\n",
       " 132,\n",
       " 134,\n",
       " 135,\n",
       " 140,\n",
       " 143,\n",
       " 144,\n",
       " 147,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 154,\n",
       " 158,\n",
       " 162,\n",
       " 168,\n",
       " 169,\n",
       " 178,\n",
       " 179,\n",
       " 181,\n",
       " 185,\n",
       " 192,\n",
       " 195,\n",
       " 196,\n",
       " 198,\n",
       " 203,\n",
       " 204,\n",
       " 209,\n",
       " 218,\n",
       " 220,\n",
       " 221,\n",
       " 223,\n",
       " 229,\n",
       " 234,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 254,\n",
       " 257,\n",
       " 259,\n",
       " 260,\n",
       " 262,\n",
       " 263,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 270,\n",
       " 275,\n",
       " 276,\n",
       " 278,\n",
       " 280,\n",
       " 281,\n",
       " 283,\n",
       " 287,\n",
       " 291,\n",
       " 292,\n",
       " 298,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 325,\n",
       " 326,\n",
       " 328,\n",
       " 332,\n",
       " 335,\n",
       " 337,\n",
       " 342,\n",
       " 345,\n",
       " 347,\n",
       " 349,\n",
       " 357,\n",
       " 359,\n",
       " 362,\n",
       " 363,\n",
       " 372,\n",
       " 373,\n",
       " 377,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 386,\n",
       " 387,\n",
       " 394,\n",
       " 397,\n",
       " 402,\n",
       " 404,\n",
       " 405,\n",
       " 408,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 427,\n",
       " 432,\n",
       " 434,\n",
       " 438,\n",
       " 441,\n",
       " 452,\n",
       " 454,\n",
       " 457,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 464,\n",
       " 465,\n",
       " 467,\n",
       " 471,\n",
       " 472,\n",
       " 474,\n",
       " 476,\n",
       " 487,\n",
       " 488,\n",
       " 490,\n",
       " 496,\n",
       " 503,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 527,\n",
       " 531,\n",
       " 555,\n",
       " 559,\n",
       " 563,\n",
       " 569,\n",
       " 576,\n",
       " 577,\n",
       " 579,\n",
       " 580,\n",
       " 586,\n",
       " 590,\n",
       " 592,\n",
       " 594,\n",
       " 598,\n",
       " 602,\n",
       " 609,\n",
       " 611,\n",
       " 617,\n",
       " 619,\n",
       " 620,\n",
       " 624,\n",
       " 628,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 636,\n",
       " 637,\n",
       " 644,\n",
       " 646,\n",
       " 650,\n",
       " 651,\n",
       " 655,\n",
       " 661,\n",
       " 671,\n",
       " 673,\n",
       " 674,\n",
       " 676,\n",
       " 678,\n",
       " 682,\n",
       " 683,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 699,\n",
       " 700,\n",
       " 707,\n",
       " 710,\n",
       " 718,\n",
       " 720,\n",
       " 724,\n",
       " 725,\n",
       " 730,\n",
       " 733,\n",
       " 737,\n",
       " 739,\n",
       " 741,\n",
       " 742,\n",
       " 746,\n",
       " 752,\n",
       " 759,\n",
       " 769,\n",
       " 770,\n",
       " 773,\n",
       " 776,\n",
       " 780,\n",
       " 786,\n",
       " 790,\n",
       " 800,\n",
       " 814,\n",
       " 815,\n",
       " 819,\n",
       " 822,\n",
       " 823,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 830,\n",
       " 831,\n",
       " 834,\n",
       " 836,\n",
       " 840,\n",
       " 853,\n",
       " 854,\n",
       " 863,\n",
       " 866,\n",
       " 867,\n",
       " 869,\n",
       " 870,\n",
       " 890,\n",
       " 911,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 919,\n",
       " 921,\n",
       " 932,\n",
       " 933,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 943,\n",
       " 951,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 963,\n",
       " 966,\n",
       " 969,\n",
       " 971,\n",
       " 974,\n",
       " 979,\n",
       " 983,\n",
       " 987,\n",
       " 992,\n",
       " 997,\n",
       " 998,\n",
       " 1001,\n",
       " 1002,\n",
       " 1008,\n",
       " 1010,\n",
       " 1012,\n",
       " 1013,\n",
       " 1018,\n",
       " 1022,\n",
       " 1031,\n",
       " 1032,\n",
       " 1033,\n",
       " 1034,\n",
       " 1039,\n",
       " 1040,\n",
       " 1044,\n",
       " 1045,\n",
       " 1048,\n",
       " 1049,\n",
       " 1055,\n",
       " 1057,\n",
       " 1058,\n",
       " 1061,\n",
       " 1062,\n",
       " 1064,\n",
       " 1066,\n",
       " 1067,\n",
       " 1068,\n",
       " 1069,\n",
       " 1073,\n",
       " 1075,\n",
       " 1095,\n",
       " 1097,\n",
       " 1098,\n",
       " 1101,\n",
       " 1102,\n",
       " 1106,\n",
       " 1114,\n",
       " 1117,\n",
       " 1119,\n",
       " 1121,\n",
       " 1126,\n",
       " 1130,\n",
       " 1132,\n",
       " 1136,\n",
       " 1150,\n",
       " 1152,\n",
       " 1154,\n",
       " 1155,\n",
       " 1156,\n",
       " 1157,\n",
       " 1160,\n",
       " 1166,\n",
       " 1168,\n",
       " 1171,\n",
       " 1180,\n",
       " 1183,\n",
       " 1189,\n",
       " 1192,\n",
       " 1198,\n",
       " 1202,\n",
       " 1204,\n",
       " 1214,\n",
       " 1220,\n",
       " 1222,\n",
       " 1223,\n",
       " 1224,\n",
       " 1225,\n",
       " 1229,\n",
       " 1230,\n",
       " 1234,\n",
       " 1241,\n",
       " 1247,\n",
       " 1251,\n",
       " 1262,\n",
       " 1264,\n",
       " 1271,\n",
       " 1272,\n",
       " 1275,\n",
       " 1285,\n",
       " 1288,\n",
       " 1291,\n",
       " 1298,\n",
       " 1299,\n",
       " 1301,\n",
       " 1303,\n",
       " 1304,\n",
       " 1305,\n",
       " 1306,\n",
       " 1309,\n",
       " 1315,\n",
       " 1317,\n",
       " 1319,\n",
       " 1324,\n",
       " 1326,\n",
       " 1327,\n",
       " 1329,\n",
       " 1334,\n",
       " 1338,\n",
       " 1340,\n",
       " 1342,\n",
       " 1347,\n",
       " 1348,\n",
       " 1354,\n",
       " 1357,\n",
       " 1358,\n",
       " 1360,\n",
       " 1366,\n",
       " 1368,\n",
       " 1369,\n",
       " 1370,\n",
       " 1371,\n",
       " 1376,\n",
       " 1385,\n",
       " 1386,\n",
       " 1388,\n",
       " 1391,\n",
       " 1392,\n",
       " 1397,\n",
       " 1399,\n",
       " 1404,\n",
       " 1407,\n",
       " 1412,\n",
       " 1416,\n",
       " 1417,\n",
       " 1419,\n",
       " 1420,\n",
       " 1439,\n",
       " 1442,\n",
       " 1450,\n",
       " 1452,\n",
       " 1465,\n",
       " 1466,\n",
       " 1468,\n",
       " 1470,\n",
       " 1471,\n",
       " 1478,\n",
       " 1479,\n",
       " 1482,\n",
       " 1487,\n",
       " 1488,\n",
       " 1490,\n",
       " 1491,\n",
       " 1492,\n",
       " 1494,\n",
       " 1495,\n",
       " 1496,\n",
       " 1503,\n",
       " 1507,\n",
       " 1511,\n",
       " 1515,\n",
       " 1521,\n",
       " 1522,\n",
       " 1523,\n",
       " 1525,\n",
       " 1526,\n",
       " 1527,\n",
       " 1528,\n",
       " 1533,\n",
       " 1535,\n",
       " 1537,\n",
       " 1538,\n",
       " 1540,\n",
       " 1542,\n",
       " 1544,\n",
       " 1546,\n",
       " 1547,\n",
       " 1552,\n",
       " 1553,\n",
       " 1555,\n",
       " 1559,\n",
       " 1560,\n",
       " 1561,\n",
       " 1568,\n",
       " 1589,\n",
       " 1592,\n",
       " 1595,\n",
       " 1606,\n",
       " 1610,\n",
       " 1615,\n",
       " 1616,\n",
       " 1617,\n",
       " 1620,\n",
       " 1624,\n",
       " 1627,\n",
       " 1632,\n",
       " 1637,\n",
       " 1638,\n",
       " 1643,\n",
       " 1645,\n",
       " 1646,\n",
       " 1650,\n",
       " 1655,\n",
       " 1663,\n",
       " 1665,\n",
       " 1666,\n",
       " 1668,\n",
       " 1675,\n",
       " 1677,\n",
       " 1678,\n",
       " 1681,\n",
       " 1684,\n",
       " 1685,\n",
       " 1686,\n",
       " 1689,\n",
       " 1690,\n",
       " 1693,\n",
       " 1694,\n",
       " 1695,\n",
       " 1696,\n",
       " 1697,\n",
       " 1699,\n",
       " 1700,\n",
       " 1701,\n",
       " 1703,\n",
       " 1708,\n",
       " 1709,\n",
       " 1712,\n",
       " 1713,\n",
       " 1715,\n",
       " 1718,\n",
       " 1720,\n",
       " 1721,\n",
       " 1722,\n",
       " 1728,\n",
       " 1729,\n",
       " 1731,\n",
       " 1734,\n",
       " 1735,\n",
       " 1736,\n",
       " 1738,\n",
       " 1739,\n",
       " 1740,\n",
       " 1741,\n",
       " 1750,\n",
       " 1755,\n",
       " 1756,\n",
       " 1768,\n",
       " 1769,\n",
       " 1770,\n",
       " 1774,\n",
       " 1777,\n",
       " 1778,\n",
       " 1780,\n",
       " 1782,\n",
       " 1791,\n",
       " 1792,\n",
       " 1796,\n",
       " 1806,\n",
       " 1808,\n",
       " 1809,\n",
       " 1812,\n",
       " 1813,\n",
       " 1818,\n",
       " 1821,\n",
       " 1822,\n",
       " 1823,\n",
       " 1826,\n",
       " 1827,\n",
       " 1841,\n",
       " 1842,\n",
       " 1844,\n",
       " 1845,\n",
       " 1846,\n",
       " 1848,\n",
       " 1853,\n",
       " 1855,\n",
       " 1861,\n",
       " 1864,\n",
       " 1865,\n",
       " 1868,\n",
       " 1869,\n",
       " 1879,\n",
       " 1880,\n",
       " 1883,\n",
       " 1884,\n",
       " 1888,\n",
       " 1892,\n",
       " 1894,\n",
       " 1897,\n",
       " 1914,\n",
       " 1916,\n",
       " 1918,\n",
       " 1926,\n",
       " 1930,\n",
       " 1932,\n",
       " 1939,\n",
       " 1942,\n",
       " 1948,\n",
       " 1950,\n",
       " 1951,\n",
       " 1953,\n",
       " 1958,\n",
       " 1959,\n",
       " 1962,\n",
       " 1970,\n",
       " 1975,\n",
       " 1976,\n",
       " 1977,\n",
       " 1979,\n",
       " 1988,\n",
       " 1990,\n",
       " 1993,\n",
       " 1996,\n",
       " 1997,\n",
       " 2003,\n",
       " 2005,\n",
       " 2009,\n",
       " 2011,\n",
       " 2015,\n",
       " 2016,\n",
       " 2021,\n",
       " 2027,\n",
       " 2028,\n",
       " 2031,\n",
       " 2033,\n",
       " 2036,\n",
       " 2037,\n",
       " 2038,\n",
       " 2039,\n",
       " 2042,\n",
       " 2043,\n",
       " 2051,\n",
       " 2069,\n",
       " 2070,\n",
       " 2071,\n",
       " 2073,\n",
       " 2086,\n",
       " 2087,\n",
       " 2091,\n",
       " 2098,\n",
       " 2099,\n",
       " 2103,\n",
       " 2106,\n",
       " 2108,\n",
       " 2113,\n",
       " 2120,\n",
       " 2121,\n",
       " 2123,\n",
       " 2125,\n",
       " 2127,\n",
       " 2128,\n",
       " 2133,\n",
       " 2135,\n",
       " 2136,\n",
       " 2140,\n",
       " 2143,\n",
       " 2145,\n",
       " 2148,\n",
       " 2153,\n",
       " 2155,\n",
       " 2157,\n",
       " 2159,\n",
       " 2163,\n",
       " 2165,\n",
       " 2171,\n",
       " 2178,\n",
       " 2179,\n",
       " 2180,\n",
       " 2181,\n",
       " 2186,\n",
       " 2189,\n",
       " 2199,\n",
       " 2206,\n",
       " 2209,\n",
       " 2210,\n",
       " 2211,\n",
       " 2212,\n",
       " 2214,\n",
       " 2215,\n",
       " 2221,\n",
       " 2233,\n",
       " 2237,\n",
       " 2238,\n",
       " 2240,\n",
       " 2242,\n",
       " 2245,\n",
       " 2251,\n",
       " 2253,\n",
       " 2256,\n",
       " 2262,\n",
       " 2263,\n",
       " 2267,\n",
       " 2278,\n",
       " 2279,\n",
       " 2283,\n",
       " 2288,\n",
       " 2296,\n",
       " 2301,\n",
       " 2303,\n",
       " 2304,\n",
       " 2308,\n",
       " 2319,\n",
       " 2320,\n",
       " 2321,\n",
       " 2322,\n",
       " 2326,\n",
       " 2327,\n",
       " 2329,\n",
       " 2333,\n",
       " 2334,\n",
       " 2336,\n",
       " 2338,\n",
       " 2340,\n",
       " 2342,\n",
       " 2346,\n",
       " 2348,\n",
       " 2358,\n",
       " 2363,\n",
       " 2364,\n",
       " 2368,\n",
       " 2372,\n",
       " 2381,\n",
       " 2385,\n",
       " 2392,\n",
       " 2393,\n",
       " 2395,\n",
       " 2406,\n",
       " 2407,\n",
       " 2408,\n",
       " 2411,\n",
       " 2412,\n",
       " 2414,\n",
       " 2416,\n",
       " 2418,\n",
       " 2423,\n",
       " 2429,\n",
       " 2433,\n",
       " 2435,\n",
       " 2440,\n",
       " 2441,\n",
       " 2444,\n",
       " 2447,\n",
       " 2450,\n",
       " 2451,\n",
       " 2452,\n",
       " 2459,\n",
       " 2462,\n",
       " 2472,\n",
       " 2476,\n",
       " 2477,\n",
       " 2478,\n",
       " 2479,\n",
       " 2480,\n",
       " 2481,\n",
       " 2488,\n",
       " 2489,\n",
       " 2490,\n",
       " 2494,\n",
       " 2499,\n",
       " 2500,\n",
       " 2501,\n",
       " 2505,\n",
       " 2509,\n",
       " 2510,\n",
       " 2511,\n",
       " 2514,\n",
       " 2516,\n",
       " 2517,\n",
       " 2518,\n",
       " 2521,\n",
       " 2522,\n",
       " 2525,\n",
       " 2527,\n",
       " 2528,\n",
       " 2533,\n",
       " 2535,\n",
       " 2536,\n",
       " 2540,\n",
       " 2541,\n",
       " 2542,\n",
       " 2543,\n",
       " 2545,\n",
       " 2547,\n",
       " 2548,\n",
       " 2549,\n",
       " 2550,\n",
       " 2555,\n",
       " 2556,\n",
       " 2560,\n",
       " 2561,\n",
       " 2562,\n",
       " 2564,\n",
       " 2571,\n",
       " 2573,\n",
       " 2576,\n",
       " 2577,\n",
       " 2578,\n",
       " 2582,\n",
       " 2584,\n",
       " 2587,\n",
       " 2588,\n",
       " 2590,\n",
       " 2593,\n",
       " 2596,\n",
       " 2599,\n",
       " 2607,\n",
       " 2622,\n",
       " 2626,\n",
       " 2629,\n",
       " 2639,\n",
       " 2643,\n",
       " 2644,\n",
       " 2651,\n",
       " 2652,\n",
       " 2653,\n",
       " 2654,\n",
       " 2657,\n",
       " 2659,\n",
       " 2671,\n",
       " 2672,\n",
       " 2673,\n",
       " 2678,\n",
       " 2680,\n",
       " 2685,\n",
       " 2689,\n",
       " 2691,\n",
       " 2692,\n",
       " 2695,\n",
       " 2697,\n",
       " 2707]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = np.ones(len(graph.x), dtype=int)\n",
    "test_mask = np.zeros(len(graph.x), dtype=int)\n",
    "test_mask[valid_nodes] = 1\n",
    "train_mask -= test_mask\n",
    "graph.train_mask = torch.tensor(train_mask, dtype=torch.bool)\n",
    "graph.test_mask = torch.tensor(test_mask, dtype = torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "input_dim = graph.num_node_features # 1433\n",
    "hidden_dim = 64\n",
    "output_dim = len(np.unique(graph.y)) # 7\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (sage): GraphSAGE(1433, 64, num_layers=2)\n",
      "  (lin): Linear(in_features=64, out_features=7, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "model = Model(in_channels=input_dim, hidden_channels=hidden_dim, out_channels=output_dim, num_layers=num_layers).to(args.device)\n",
    "print(model)\n",
    "# Model optimizer, may change into adam\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "import copy\n",
    "graph = graph.to(args.device, 'x', 'y')\n",
    "train_loader = NeighborLoader(\n",
    "\tgraph, \n",
    "\tnum_neighbors=[args.num_neg_samples] * (num_layers - 1),\n",
    "\tinput_nodes=graph.train_mask,\n",
    "\tshuffle=True,\n",
    "\tbatch_size=args.batch_size)\n",
    "valid_loader = NeighborLoader(\n",
    "\tcopy.copy(graph),\n",
    "\tinput_nodes = None,\n",
    "\tnum_neighbors=[-1],\n",
    "\t# num_neighbors=[args.num_neg_samples] * (num_layers - 1),\n",
    "\tshuffle = False,\n",
    "\tbatch_size = args.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "from tqdm import tqdm\n",
    "def train(epoch, verbose=False):\n",
    "\tmodel.train()\n",
    "\tif verbose:\n",
    "\t\tpbar = tqdm(total=int(len(train_loader.dataset)))\n",
    "\t\tpbar.set_description(f'Epoch{ epoch:02d}')\n",
    "\ttotal_loss = 0\n",
    "\ttotal_correct = 0\n",
    "\ttotal_examples = 0\n",
    "\tfor batch in train_loader:\n",
    "\t\tbatch = batch.to(args.device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\ty_pred = (model.forward(batch)[:batch.batch_size])[batch.train_mask[:batch.batch_size]].argmax(dim=-1)\n",
    "\t\ty_true = (batch.y[:batch.batch_size])[batch.train_mask[:batch.batch_size]]\n",
    "\t\ttotal_correct += int((y_pred == y_true).sum())\n",
    "\t\t# print(y_pred.unique(), y_true.unique())\n",
    "\t\tloss = model.loss(batch)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\tloss = loss.data.item()\n",
    "\t\ttotal_loss += loss * batch.train_mask[:batch.batch_size].sum()\n",
    "\t\ttotal_examples += batch.train_mask[:batch.batch_size].sum()\n",
    "\t\tif verbose:\n",
    "\t\t\tpbar.update(batch.batch_size)\n",
    "\tif verbose:\n",
    "\t\tpbar.close()\n",
    "\treturn total_loss/total_examples, total_correct / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Loss: 1.9365, Train accuracy: 0.2510\n",
      "Epoch 02, Loss: 1.8933, Train accuracy: 0.3229\n",
      "Epoch 03, Loss: 1.8017, Train accuracy: 0.2993\n",
      "Epoch 04, Loss: 1.6706, Train accuracy: 0.3014\n",
      "Epoch 05, Loss: 1.5265, Train accuracy: 0.3868\n",
      "Epoch 06, Loss: 1.3461, Train accuracy: 0.5121\n",
      "Epoch 07, Loss: 1.1471, Train accuracy: 0.6488\n",
      "Epoch 08, Loss: 0.9676, Train accuracy: 0.7510\n",
      "Epoch 09, Loss: 0.8179, Train accuracy: 0.8082\n",
      "Epoch 10, Loss: 0.6950, Train accuracy: 0.8417\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.num_epochs+1):\n",
    "    avg_loss, acc = train(epoch)\n",
    "    print(f'Epoch {epoch:02d}, Loss: {avg_loss:.4f}, Train accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test():\n",
    "\tmodel.eval()\n",
    "\t# total_correct = 0\n",
    "\t# total_examples = 0\n",
    "\ty_pred_all = []\n",
    "\ty_true_all = []\n",
    "\tfor batch in valid_loader:\n",
    "\t\tbatch = batch.to(args.device)\n",
    "\t\ty_pred = model.forward(batch)[:batch.batch_size]\n",
    "\t\ty_true = batch.y[:batch.batch_size]\n",
    "\t\ty_pred_val = y_pred[batch.test_mask[:batch.batch_size]].argmax(dim=-1).cpu()\n",
    "\t\ty_true_val = y_true[batch.test_mask[:batch.batch_size]].cpu()\n",
    "\t\t# total_correct += int((y_pred_val.argmax(dim=-1) == y_true_val).sum())\n",
    "\t\t# total_examples += batch.test_mask[:batch.batch_size].sum()\n",
    "\t\t# print(total_correct, total_examples)\n",
    "\t\ty_pred_all.append(y_pred_val)\n",
    "\t\ty_true_all.append(y_true_val)\n",
    "\ty_pred_all = torch.cat(y_pred_all)\n",
    "\ty_true_all = torch.cat(y_true_all)\n",
    "\tprint(\"Validation Macro F1:\" +  str(np.round(f1_score(y_true_all, y_pred_all, average=\"macro\"), 6)))\n",
    "\tprint(\"Validation Micro F1:\" +  str(np.round(f1_score(y_true_all, y_pred_all, average=\"micro\"), 6)))\n",
    "\n",
    "\tval_acc = (y_pred_all == y_true_all).sum()/(len(y_pred_all))\n",
    "\tprint(f\"Val accuracy: {val_acc: .4f}\")\n",
    "\treturn y_pred_all, y_true_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Macro F1:0.727744\n",
      "Validation Micro F1:0.785\n",
      "Val accuracy:  0.7850\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_true = test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
